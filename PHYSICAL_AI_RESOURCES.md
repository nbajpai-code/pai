# Physical AI (PAI) Resources

A comprehensive, curated list of resources for **Physical AI**, the next frontier where artificial intelligence meets the physical world through robotics, autonomous agents, and foundation models.

## üè¢ Top Enterprises & Startups (2025-2026)

### Leaders
*   **NVIDIA**: The infrastructure backbone.
    *   *Key Projects*: Project GR00T (Humanoid Foundation Model), Isaac Sim, Cosmos World Foundation Models (WFMs).
    *   *Focus*: Simulation, accelerate compute, and open foundation models for robots.
*   **Tesla**:
    *   *Key Project*: **Optimus** (Gen 2/3).
    *   *Focus*: Mass manufacturing of humanoid robots for factories and eventually homes. 
*   **Figure AI**:
    *   *Key Projects*: Figure 01, Figure 02.
    *   *Partners*: OpenAI (for brain/reasoning), BMW (manufacturing pilot).
    *   *Focus*: General purpose humanoid robotics.
*   **Physical Intelligence (Pi)**:
    *   *Focus*: Developing "brains" for robots (Foundation Models) rather than hardware.
    *   *Key Model*: **œÄ0 (pi-zero)** - A flow-matching VLA model.
*   **1X Technologies**:
    *   *Key Projects*: NEO (consumer android), EVE (security/logistics).
    *   *Backers*: OpenAI, Tiger Global.

### Emerging & Specialized
*   **Boston Dynamics** (Hyundai): Atlas (Electric), Spot.
*   **Agility Robotics**: Digit (Logistics focus).
*   **Sanctuary AI**: Phoenix (General purpose human-like intelligence).
*   **Skild AI**: Building a shared "general purpose brain" for diverse robot embodiments.
*   **Covariant**: Foundation models for industrial manipulation (RFM-1).

---

## üìÑ Key Research Papers (Arxiv 2024-2025)

### Foundation Models & Reasoners
*   **Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning** (NVIDIA, 2025)
    *   *Significance*: Exploring how LLMs/VLMs facilitate spatial understanding and object permanence.
*   **OpenVLA: An Open-Source Vision-Language-Action Model** (Stanford, 2024)
    *   *Significance*: Open-source VLA model trained on the Open X-Embodiment dataset.
*   **œÄ0: A Flow-Matching Foundation Model for Autoregressive Actions** (Physical Intelligence, 2024/2025)
    *   *Significance*: Achieving high-frequency continuous control.
*   **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control** (Google DeepMind)
    *   *Significance*: Proving that LLMs can transfer semantic knowledge to robotic actions.
*   **AutoRT: Embodied Foundation Models for Large-Scale Orchestration** (Google DeepMind)
    *   *Significance*: Coordinating fleets of robots using VLM-based planners.

### Simulation & World Models
*   **World Simulation with Video Foundation Models (Cosmos-Predict)** (NVIDIA, 2025)
    *   *Focus*: Generating realistic future video frames to simulate outcomes for robots.

---

## üíª Open Source Repositories & Code

### Models & Frameworks
*   **[openpi](https://github.com/Physical-Intelligence/openpi)**: Official repository for **œÄ0** models (Vision-Language-Action). Implements flow matching for robotic control.
*   **[OpenVLA](https://github.com/openvla/openvla)**: Fine-tuning Llama/Prismatic models for robotic manipulation tasks.
*   **[LeRobot](https://github.com/huggingface/lerobot)**: Hugging Face's open robotics initiative. Aims to be the "Transformers" library for robotics.
*   **[act](https://github.com/tonyzhaozh/act)**: Action Chunking with Transformers (ALOHA).
*   **[droid](https://github.com/droid-dataset/droid)**: Distributed Robot Interaction Dataset.

### Simulators
*   **NVIDIA Isaac Lab**: Modular framework for robot learning based on Isaac Sim.
*   **Mujoco**: Physics engine (acquired by DeepMind) widely used for RL research.
*   **Genesis**: A generative world and diverse physics simulator.

---

## üåê Internet / Meta / xAI
*   **Meta FAIR**: actively researching **V-JEPA** (Video Joint Embedding Predictive Architecture) for non-generative world modeling, crucial for physical understanding.
*   **xAI**: Partnering with Tesla to integrate Grok's reasoning capabilities into Optimus.
*   **Hugging Face**: Launching the *LeRobot* community to democratize physical AI data and models.

---

## üîÑ Dynamic Updates
*Last Updated: 2026-02-12 08:37:53 UTC*

### üìÑ Latest Research (Arxiv)
*   [APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots](http://arxiv.org/abs/2602.11143v1) (2026-02-11)
*   [PhyCritic: Multimodal Critic Models for Physical AI](http://arxiv.org/abs/2602.11124v1) (2026-02-11)
*   [RISE: Self-Improving Robot Policy with Compositional World Model](http://arxiv.org/abs/2602.11075v1) (2026-02-11)
*   [Scaling World Model for Hierarchical Manipulation Policies](http://arxiv.org/abs/2602.10983v1) (2026-02-11)
*   [RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation](http://arxiv.org/abs/2602.10980v1) (2026-02-11)
